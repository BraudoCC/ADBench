{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-step Guidence on How to Install and Use ADBench**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install ADBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T14:25:26.100338Z",
     "start_time": "2023-07-19T14:25:13.171898Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Collecting adbench\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/20/c1/0fd05725cdde00cfa235ad43fc104b9ba7a871190f6027fc00c51e0a1558/adbench-0.1.0-py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: barbar in f:\\anaconda\\lib\\site-packages (from adbench) (0.2.1)\n",
      "Requirement already satisfied: rtdl in f:\\anaconda\\lib\\site-packages (from adbench) (0.0.13)\n",
      "Requirement already satisfied: lightgbm in f:\\anaconda\\lib\\site-packages (from adbench) (2.3.1)\n",
      "Requirement already satisfied: keras in f:\\anaconda\\lib\\site-packages (from adbench) (2.8.0)\n",
      "Requirement already satisfied: tensorflow in f:\\anaconda\\lib\\site-packages (from adbench) (2.8.0)\n",
      "Requirement already satisfied: combo in f:\\anaconda\\lib\\site-packages (from adbench) (0.1.2)\n",
      "Requirement already satisfied: copulas in f:\\anaconda\\lib\\site-packages (from adbench) (0.7.0)\n",
      "Requirement already satisfied: fsspec in f:\\anaconda\\lib\\site-packages (from adbench) (2021.10.0)\n",
      "Requirement already satisfied: delu in f:\\anaconda\\lib\\site-packages (from adbench) (0.0.10)\n",
      "Requirement already satisfied: scikit-learn in f:\\anaconda\\lib\\site-packages (from adbench) (0.20.3)\n",
      "Requirement already satisfied: catboost in f:\\anaconda\\lib\\site-packages (from adbench) (1.0.6)\n",
      "Requirement already satisfied: torch in f:\\anaconda\\lib\\site-packages (from adbench) (1.10.1)\n",
      "Requirement already satisfied: xgboost in f:\\anaconda\\lib\\site-packages (from adbench) (1.2.0)\n",
      "Requirement already satisfied: pyod==1.0.0 in f:\\anaconda\\lib\\site-packages (from adbench) (1.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.18 in f:\\anaconda\\lib\\site-packages (from rtdl->adbench) (1.21.6)\n",
      "Requirement already satisfied: scipy in f:\\anaconda\\lib\\site-packages (from lightgbm->adbench) (1.7.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (14.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.10.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.0)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (65.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.31.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (3.13.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.2.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.1.0)\n",
      "Requirement already satisfied: numba>=0.35 in f:\\anaconda\\lib\\site-packages (from combo->adbench) (0.51.2)\n",
      "Requirement already satisfied: joblib in f:\\anaconda\\lib\\site-packages (from combo->adbench) (0.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\minqi\\appdata\\roaming\\python\\python37\\site-packages (from combo->adbench) (3.5.3)\n",
      "Requirement already satisfied: pandas<2,>=1.1.3 in f:\\anaconda\\lib\\site-packages (from copulas->adbench) (1.3.5)\n",
      "Requirement already satisfied: pynvml<12,>=11.0 in f:\\anaconda\\lib\\site-packages (from delu->adbench) (11.4.1)\n",
      "Requirement already satisfied: tqdm<5,>=4.0 in f:\\anaconda\\lib\\site-packages (from delu->adbench) (4.62.3)\n",
      "Requirement already satisfied: plotly in f:\\anaconda\\lib\\site-packages (from catboost->adbench) (4.14.3)\n",
      "Requirement already satisfied: graphviz in f:\\anaconda\\lib\\site-packages (from catboost->adbench) (0.20)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\minqi\\appdata\\roaming\\python\\python37\\site-packages (from pyod==1.0.0->adbench) (0.13.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (2.24.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (1.20.1)\n",
      "Requirement already satisfied: wheel>=0.26 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (0.35.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (3.2.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in f:\\anaconda\\lib\\site-packages (from numba>=0.35->combo->adbench) (0.34.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\minqi\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib->combo->adbench) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in f:\\anaconda\\lib\\site-packages (from pandas<2,>=1.1.3->copulas->adbench) (2020.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in f:\\anaconda\\lib\\site-packages (from tqdm<5,>=4.0->delu->adbench) (0.4.3)\n",
      "Requirement already satisfied: retrying>=1.3.3 in f:\\anaconda\\lib\\site-packages (from plotly->catboost->adbench) (1.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.2 in f:\\anaconda\\lib\\site-packages (from statsmodels->pyod==1.0.0->adbench) (0.5.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (2022.12.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (1.23)\n",
      "Requirement already satisfied: idna<3,>=2.5 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (2.10)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in f:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in f:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in f:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in f:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->adbench) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in f:\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->adbench) (6.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in f:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in f:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->adbench) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in f:\\anaconda\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->adbench) (3.1.0)\n",
      "Installing collected packages: adbench\n",
      "Successfully installed adbench-0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already up-to-date: adbench in f:\\anaconda\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: barbar in f:\\anaconda\\lib\\site-packages (from adbench) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: combo in f:\\anaconda\\lib\\site-packages (from adbench) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: rtdl in f:\\anaconda\\lib\\site-packages (from adbench) (0.0.13)\n",
      "Requirement already satisfied, skipping upgrade: torch in f:\\anaconda\\lib\\site-packages (from adbench) (1.10.1)\n",
      "Requirement already satisfied, skipping upgrade: lightgbm in f:\\anaconda\\lib\\site-packages (from adbench) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: catboost in f:\\anaconda\\lib\\site-packages (from adbench) (1.0.6)\n",
      "Requirement already satisfied, skipping upgrade: delu in f:\\anaconda\\lib\\site-packages (from adbench) (0.0.10)\n",
      "Requirement already satisfied, skipping upgrade: copulas in f:\\anaconda\\lib\\site-packages (from adbench) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow in f:\\anaconda\\lib\\site-packages (from adbench) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: keras in f:\\anaconda\\lib\\site-packages (from adbench) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: xgboost in f:\\anaconda\\lib\\site-packages (from adbench) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec in f:\\anaconda\\lib\\site-packages (from adbench) (2021.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyod==1.0.0 in f:\\anaconda\\lib\\site-packages (from adbench) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in f:\\anaconda\\lib\\site-packages (from adbench) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib in f:\\anaconda\\lib\\site-packages (from combo->adbench) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: numba>=0.35 in f:\\anaconda\\lib\\site-packages (from combo->adbench) (0.51.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in f:\\anaconda\\lib\\site-packages (from combo->adbench) (1.7.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13 in f:\\anaconda\\lib\\site-packages (from combo->adbench) (1.21.6)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in c:\\users\\minqi\\appdata\\roaming\\python\\python37\\site-packages (from combo->adbench) (3.5.3)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in f:\\anaconda\\lib\\site-packages (from torch->adbench) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: plotly in f:\\anaconda\\lib\\site-packages (from catboost->adbench) (4.14.3)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.24.0 in f:\\anaconda\\lib\\site-packages (from catboost->adbench) (1.3.5)\n",
      "Requirement already satisfied, skipping upgrade: six in f:\\anaconda\\lib\\site-packages (from catboost->adbench) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: graphviz in f:\\anaconda\\lib\\site-packages (from catboost->adbench) (0.20)\n",
      "Requirement already satisfied, skipping upgrade: pynvml<12,>=11.0 in f:\\anaconda\\lib\\site-packages (from delu->adbench) (11.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5,>=4.0 in f:\\anaconda\\lib\\site-packages (from delu->adbench) (4.62.3)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers>=1.12 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-io-gcs-filesystem>=0.23.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.24.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.31.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (65.5.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.9,>=2.8 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.4.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: libclang>=9.0.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (14.0.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.1 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: tf-estimator-nightly==2.8.0.dev2021122109 in f:\\anaconda\\lib\\site-packages (from tensorflow->adbench) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied, skipping upgrade: statsmodels in c:\\users\\minqi\\appdata\\roaming\\python\\python37\\site-packages (from pyod==1.0.0->adbench) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: llvmlite<0.35,>=0.34.0.dev0 in f:\\anaconda\\lib\\site-packages (from numba>=0.35->combo->adbench) (0.34.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in c:\\users\\minqi\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib->combo->adbench) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: fonttools>=4.22.0 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (4.38.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (9.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in f:\\anaconda\\lib\\site-packages (from matplotlib->combo->adbench) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in f:\\anaconda\\lib\\site-packages (from plotly->catboost->adbench) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in f:\\anaconda\\lib\\site-packages (from pandas>=0.24.0->catboost->adbench) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama; platform_system == \"Windows\" in f:\\anaconda\\lib\\site-packages (from tqdm<5,>=4.0->delu->adbench) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: wheel<1.0,>=0.23.0 in f:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->adbench) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (1.20.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in f:\\anaconda\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->adbench) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: patsy>=0.5.2 in f:\\anaconda\\lib\\site-packages (from statsmodels->pyod==1.0.0->adbench) (0.5.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (2022.12.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in f:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->adbench) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in f:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in f:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in f:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in f:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->adbench) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in f:\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->adbench) (6.7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in f:\\anaconda\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->adbench) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in f:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->adbench) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in f:\\anaconda\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->adbench) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install adbench\n",
    "!pip install --upgrade adbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T14:26:07.647181Z",
     "start_time": "2023-07-19T14:25:56.405345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets from the remote github repo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 2504.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_by_ResNet18 already exists. Skipping download...\n",
      "CV_by_ViT already exists. Skipping download...\n",
      "NLP_by_BERT already exists. Skipping download...\n",
      "NLP_by_RoBERTa already exists. Skipping download...\n",
      "Classical already exists. Skipping download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download datasets in ADBench from the remote github repo\n",
    "from adbench.myutils import Utils\n",
    "utils = Utils()\n",
    "utils.download_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run ADBench "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T15:33:13.848925Z",
     "start_time": "2023-07-17T15:33:13.833498Z"
    }
   },
   "source": [
    "## Run ADBench experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T14:44:03.627289Z",
     "start_time": "2023-07-19T14:26:41.551958Z"
    },
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampling for dataset 10_cover...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 97, 'Anomalies Ratio(%)': 0.97}\n",
      "subsampling for dataset 10_cover...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 116, 'Anomalies Ratio(%)': 1.16}\n",
      "subsampling for dataset 10_cover...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 95, 'Anomalies Ratio(%)': 0.95}\n",
      "subsampling for dataset 11_donors...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "subsampling for dataset 11_donors...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 578, 'Anomalies Ratio(%)': 5.78}\n",
      "subsampling for dataset 11_donors...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 597, 'Anomalies Ratio(%)': 5.97}\n",
      "current noise type: None\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "current noise type: None\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "current noise type: None\n",
      "{'Samples': 1941, 'Features': 27, 'Anomalies': 673, 'Anomalies Ratio(%)': 34.67}\n",
      "subsampling for dataset 13_fraud...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "subsampling for dataset 13_fraud...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 15, 'Anomalies Ratio(%)': 0.15}\n",
      "subsampling for dataset 13_fraud...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 29, 'Anomalies': 13, 'Anomalies Ratio(%)': 0.13}\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 42, 'Anomalies Ratio(%)': 4.2}\n",
      "generating duplicate samples for dataset 14_glass...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 7, 'Anomalies': 34, 'Anomalies Ratio(%)': 3.4}\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 170, 'Anomalies Ratio(%)': 17.0}\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 139, 'Anomalies Ratio(%)': 13.9}\n",
      "generating duplicate samples for dataset 15_Hepatitis...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 19, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "subsampling for dataset 16_http...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 46, 'Anomalies Ratio(%)': 0.46}\n",
      "subsampling for dataset 16_http...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 35, 'Anomalies Ratio(%)': 0.35}\n",
      "subsampling for dataset 16_http...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 34, 'Anomalies Ratio(%)': 0.34}\n",
      "current noise type: None\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "current noise type: None\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "current noise type: None\n",
      "{'Samples': 1966, 'Features': 1555, 'Anomalies': 368, 'Anomalies Ratio(%)': 18.72}\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 369, 'Anomalies Ratio(%)': 36.9}\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 374, 'Anomalies Ratio(%)': 37.4}\n",
      "generating duplicate samples for dataset 18_Ionosphere...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 32, 'Anomalies': 349, 'Anomalies Ratio(%)': 34.9}\n",
      "current noise type: None\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "current noise type: None\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "current noise type: None\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 1333, 'Anomalies Ratio(%)': 20.71}\n",
      "subsampling for dataset 1_ALOI...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 315, 'Anomalies Ratio(%)': 3.15}\n",
      "subsampling for dataset 1_ALOI...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 334, 'Anomalies Ratio(%)': 3.34}\n",
      "subsampling for dataset 1_ALOI...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 27, 'Anomalies': 332, 'Anomalies Ratio(%)': 3.32}\n",
      "current noise type: None\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "current noise type: None\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "current noise type: None\n",
      "{'Samples': 1600, 'Features': 32, 'Anomalies': 100, 'Anomalies Ratio(%)': 6.25}\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 38, 'Anomalies Ratio(%)': 3.8}\n",
      "generating duplicate samples for dataset 21_Lymphography...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 18, 'Anomalies': 43, 'Anomalies Ratio(%)': 4.3}\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3548, 'Anomalies Ratio(%)': 35.48}\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3533, 'Anomalies Ratio(%)': 35.33}\n",
      "subsampling for dataset 22_magic.gamma...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 10, 'Anomalies': 3500, 'Anomalies Ratio(%)': 35.0}\n",
      "subsampling for dataset 23_mammography...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 230, 'Anomalies Ratio(%)': 2.3}\n",
      "subsampling for dataset 23_mammography...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 241, 'Anomalies Ratio(%)': 2.41}\n",
      "subsampling for dataset 23_mammography...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 6, 'Anomalies': 227, 'Anomalies Ratio(%)': 2.27}\n",
      "current noise type: None\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "current noise type: None\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "current noise type: None\n",
      "{'Samples': 7603, 'Features': 100, 'Anomalies': 700, 'Anomalies Ratio(%)': 9.21}\n",
      "current noise type: None\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "current noise type: None\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "current noise type: None\n",
      "{'Samples': 3062, 'Features': 166, 'Anomalies': 97, 'Anomalies Ratio(%)': 3.17}\n",
      "current noise type: None\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "current noise type: None\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "current noise type: None\n",
      "{'Samples': 5216, 'Features': 64, 'Anomalies': 150, 'Anomalies Ratio(%)': 2.88}\n",
      "current noise type: None\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "current noise type: None\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "current noise type: None\n",
      "{'Samples': 5393, 'Features': 10, 'Anomalies': 510, 'Anomalies Ratio(%)': 9.46}\n",
      "current noise type: None\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "current noise type: None\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "current noise type: None\n",
      "{'Samples': 6870, 'Features': 16, 'Anomalies': 156, 'Anomalies Ratio(%)': 2.27}\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 367, 'Anomalies Ratio(%)': 36.7}\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 346, 'Anomalies Ratio(%)': 34.6}\n",
      "generating duplicate samples for dataset 29_Pima...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 8, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "current noise type: None\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "current noise type: None\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "current noise type: None\n",
      "{'Samples': 7200, 'Features': 6, 'Anomalies': 534, 'Anomalies Ratio(%)': 7.42}\n",
      "current noise type: None\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "current noise type: None\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n",
      "current noise type: None\n",
      "{'Samples': 6435, 'Features': 36, 'Anomalies': 2036, 'Anomalies Ratio(%)': 31.64}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current noise type: None\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "current noise type: None\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "current noise type: None\n",
      "{'Samples': 5803, 'Features': 36, 'Anomalies': 71, 'Anomalies Ratio(%)': 1.22}\n",
      "subsampling for dataset 32_shuttle...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 669, 'Anomalies Ratio(%)': 6.69}\n",
      "subsampling for dataset 32_shuttle...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 697, 'Anomalies Ratio(%)': 6.97}\n",
      "subsampling for dataset 32_shuttle...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 9, 'Anomalies': 714, 'Anomalies Ratio(%)': 7.14}\n",
      "subsampling for dataset 33_skin...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2081, 'Anomalies Ratio(%)': 20.81}\n",
      "subsampling for dataset 33_skin...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2082, 'Anomalies Ratio(%)': 20.82}\n",
      "subsampling for dataset 33_skin...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 2066, 'Anomalies Ratio(%)': 20.66}\n",
      "subsampling for dataset 34_smtp...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "subsampling for dataset 34_smtp...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 4, 'Anomalies Ratio(%)': 0.04}\n",
      "subsampling for dataset 34_smtp...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 3, 'Anomalies': 5, 'Anomalies Ratio(%)': 0.05}\n",
      "current noise type: None\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "current noise type: None\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "current noise type: None\n",
      "{'Samples': 4207, 'Features': 57, 'Anomalies': 1679, 'Anomalies Ratio(%)': 39.91}\n",
      "current noise type: None\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "current noise type: None\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "current noise type: None\n",
      "{'Samples': 3686, 'Features': 400, 'Anomalies': 61, 'Anomalies Ratio(%)': 1.65}\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 90, 'Anomalies Ratio(%)': 9.0}\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 95, 'Anomalies Ratio(%)': 9.5}\n",
      "generating duplicate samples for dataset 37_Stamps...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 100, 'Anomalies Ratio(%)': 10.0}\n",
      "current noise type: None\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "current noise type: None\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "current noise type: None\n",
      "{'Samples': 3772, 'Features': 6, 'Anomalies': 93, 'Anomalies Ratio(%)': 2.47}\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 123, 'Anomalies Ratio(%)': 12.3}\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "generating duplicate samples for dataset 39_vertebral...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 6, 'Anomalies': 133, 'Anomalies Ratio(%)': 13.3}\n",
      "subsampling for dataset 3_backdoor...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 252, 'Anomalies Ratio(%)': 2.52}\n",
      "subsampling for dataset 3_backdoor...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 246, 'Anomalies Ratio(%)': 2.46}\n",
      "subsampling for dataset 3_backdoor...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 196, 'Anomalies': 256, 'Anomalies Ratio(%)': 2.56}\n",
      "current noise type: None\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "current noise type: None\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "current noise type: None\n",
      "{'Samples': 1456, 'Features': 12, 'Anomalies': 50, 'Anomalies Ratio(%)': 3.43}\n",
      "current noise type: None\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "current noise type: None\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "current noise type: None\n",
      "{'Samples': 3443, 'Features': 21, 'Anomalies': 100, 'Anomalies Ratio(%)': 2.9}\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 35, 'Anomalies Ratio(%)': 3.5}\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 45, 'Anomalies Ratio(%)': 4.5}\n",
      "generating duplicate samples for dataset 42_WBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 48, 'Anomalies Ratio(%)': 4.8}\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 26, 'Anomalies Ratio(%)': 2.6}\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 29, 'Anomalies Ratio(%)': 2.9}\n",
      "generating duplicate samples for dataset 43_WDBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 30, 'Anomalies': 27, 'Anomalies Ratio(%)': 2.7}\n",
      "current noise type: None\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "current noise type: None\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "current noise type: None\n",
      "{'Samples': 4819, 'Features': 5, 'Anomalies': 257, 'Anomalies Ratio(%)': 5.33}\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 75, 'Anomalies Ratio(%)': 7.5}\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 80, 'Anomalies Ratio(%)': 8.0}\n",
      "generating duplicate samples for dataset 45_wine...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 13, 'Anomalies': 88, 'Anomalies Ratio(%)': 8.8}\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 239, 'Anomalies Ratio(%)': 23.9}\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 224, 'Anomalies Ratio(%)': 22.4}\n",
      "generating duplicate samples for dataset 46_WPBC...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 33, 'Anomalies': 225, 'Anomalies Ratio(%)': 22.5}\n",
      "current noise type: None\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "current noise type: None\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "current noise type: None\n",
      "{'Samples': 1484, 'Features': 8, 'Anomalies': 507, 'Anomalies Ratio(%)': 34.16}\n",
      "generating duplicate samples for dataset 4_breastw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 348, 'Anomalies Ratio(%)': 34.8}\n",
      "generating duplicate samples for dataset 4_breastw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 336, 'Anomalies Ratio(%)': 33.6}\n",
      "generating duplicate samples for dataset 4_breastw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 9, 'Anomalies': 364, 'Anomalies Ratio(%)': 36.4}\n",
      "subsampling for dataset 5_campaign...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1134, 'Anomalies Ratio(%)': 11.34}\n",
      "subsampling for dataset 5_campaign...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1129, 'Anomalies Ratio(%)': 11.29}\n",
      "subsampling for dataset 5_campaign...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 62, 'Anomalies': 1129, 'Anomalies Ratio(%)': 11.29}\n",
      "current noise type: None\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "current noise type: None\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "current noise type: None\n",
      "{'Samples': 1831, 'Features': 21, 'Anomalies': 176, 'Anomalies Ratio(%)': 9.61}\n",
      "current noise type: None\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "current noise type: None\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n",
      "current noise type: None\n",
      "{'Samples': 2114, 'Features': 21, 'Anomalies': 466, 'Anomalies Ratio(%)': 22.04}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsampling for dataset 8_celeba...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 222, 'Anomalies Ratio(%)': 2.22}\n",
      "subsampling for dataset 8_celeba...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 238, 'Anomalies Ratio(%)': 2.38}\n",
      "subsampling for dataset 8_celeba...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 39, 'Anomalies': 216, 'Anomalies Ratio(%)': 2.16}\n",
      "subsampling for dataset 9_census...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 618, 'Anomalies Ratio(%)': 6.18}\n",
      "subsampling for dataset 9_census...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 628, 'Anomalies Ratio(%)': 6.28}\n",
      "subsampling for dataset 9_census...\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 500, 'Anomalies': 652, 'Anomalies Ratio(%)': 6.52}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 5263, 'Features': 512, 'Anomalies': 263, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6315, 'Features': 512, 'Anomalies': 315, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "generating duplicate samples for dataset MVTec-AD_bottle...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 215, 'Anomalies Ratio(%)': 21.5}\n",
      "generating duplicate samples for dataset MVTec-AD_bottle...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 215, 'Anomalies Ratio(%)': 21.5}\n",
      "generating duplicate samples for dataset MVTec-AD_bottle...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 198, 'Anomalies Ratio(%)': 19.8}\n",
      "generating duplicate samples for dataset MVTec-AD_cable...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 251, 'Anomalies Ratio(%)': 25.1}\n",
      "generating duplicate samples for dataset MVTec-AD_cable...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 242, 'Anomalies Ratio(%)': 24.2}\n",
      "generating duplicate samples for dataset MVTec-AD_cable...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 242, 'Anomalies Ratio(%)': 24.2}\n",
      "generating duplicate samples for dataset MVTec-AD_capsule...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 308, 'Anomalies Ratio(%)': 30.8}\n",
      "generating duplicate samples for dataset MVTec-AD_capsule...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 304, 'Anomalies Ratio(%)': 30.4}\n",
      "generating duplicate samples for dataset MVTec-AD_capsule...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 29.4}\n",
      "generating duplicate samples for dataset MVTec-AD_carpet...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 254, 'Anomalies Ratio(%)': 25.4}\n",
      "generating duplicate samples for dataset MVTec-AD_carpet...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 227, 'Anomalies Ratio(%)': 22.7}\n",
      "generating duplicate samples for dataset MVTec-AD_carpet...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 213, 'Anomalies Ratio(%)': 21.3}\n",
      "generating duplicate samples for dataset MVTec-AD_grid...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 180, 'Anomalies Ratio(%)': 18.0}\n",
      "generating duplicate samples for dataset MVTec-AD_grid...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 169, 'Anomalies Ratio(%)': 16.9}\n",
      "generating duplicate samples for dataset MVTec-AD_grid...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 174, 'Anomalies Ratio(%)': 17.4}\n",
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n",
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 160, 'Anomalies Ratio(%)': 16.0}\n",
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 142, 'Anomalies Ratio(%)': 14.2}\n",
      "generating duplicate samples for dataset MVTec-AD_leather...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 259, 'Anomalies Ratio(%)': 25.9}\n",
      "generating duplicate samples for dataset MVTec-AD_leather...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 273, 'Anomalies Ratio(%)': 27.3}\n",
      "generating duplicate samples for dataset MVTec-AD_leather...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 246, 'Anomalies Ratio(%)': 24.6}\n",
      "generating duplicate samples for dataset MVTec-AD_metal_nut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 284, 'Anomalies Ratio(%)': 28.4}\n",
      "generating duplicate samples for dataset MVTec-AD_metal_nut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 283, 'Anomalies Ratio(%)': 28.3}\n",
      "generating duplicate samples for dataset MVTec-AD_metal_nut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 281, 'Anomalies Ratio(%)': 28.1}\n",
      "generating duplicate samples for dataset MVTec-AD_pill...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 314, 'Anomalies Ratio(%)': 31.4}\n",
      "generating duplicate samples for dataset MVTec-AD_pill...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 300, 'Anomalies Ratio(%)': 30.0}\n",
      "generating duplicate samples for dataset MVTec-AD_pill...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 326, 'Anomalies Ratio(%)': 32.6}\n",
      "generating duplicate samples for dataset MVTec-AD_screw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 240, 'Anomalies Ratio(%)': 24.0}\n",
      "generating duplicate samples for dataset MVTec-AD_screw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 244, 'Anomalies Ratio(%)': 24.4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating duplicate samples for dataset MVTec-AD_screw...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 266, 'Anomalies Ratio(%)': 26.6}\n",
      "generating duplicate samples for dataset MVTec-AD_tile...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 227, 'Anomalies Ratio(%)': 22.7}\n",
      "generating duplicate samples for dataset MVTec-AD_tile...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 231, 'Anomalies Ratio(%)': 23.1}\n",
      "generating duplicate samples for dataset MVTec-AD_tile...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 252, 'Anomalies Ratio(%)': 25.2}\n",
      "generating duplicate samples for dataset MVTec-AD_toothbrush...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 271, 'Anomalies Ratio(%)': 27.1}\n",
      "generating duplicate samples for dataset MVTec-AD_toothbrush...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 285, 'Anomalies Ratio(%)': 28.5}\n",
      "generating duplicate samples for dataset MVTec-AD_toothbrush...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 288, 'Anomalies Ratio(%)': 28.8}\n",
      "generating duplicate samples for dataset MVTec-AD_transistor...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 115, 'Anomalies Ratio(%)': 11.5}\n",
      "generating duplicate samples for dataset MVTec-AD_transistor...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 135, 'Anomalies Ratio(%)': 13.5}\n",
      "generating duplicate samples for dataset MVTec-AD_transistor...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 114, 'Anomalies Ratio(%)': 11.4}\n",
      "generating duplicate samples for dataset MVTec-AD_wood...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 184, 'Anomalies Ratio(%)': 18.4}\n",
      "generating duplicate samples for dataset MVTec-AD_wood...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 189, 'Anomalies Ratio(%)': 18.9}\n",
      "generating duplicate samples for dataset MVTec-AD_wood...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 175, 'Anomalies Ratio(%)': 17.5}\n",
      "generating duplicate samples for dataset MVTec-AD_zipper...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 305, 'Anomalies Ratio(%)': 30.5}\n",
      "generating duplicate samples for dataset MVTec-AD_zipper...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 304, 'Anomalies Ratio(%)': 30.4}\n",
      "generating duplicate samples for dataset MVTec-AD_zipper...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 320, 'Anomalies Ratio(%)': 32.0}\n",
      "current noise type: None\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5208, 'Features': 512, 'Anomalies': 260, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 512, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 8944, 'Features': 512, 'Anomalies': 447, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 7850, 'Features': 512, 'Anomalies': 392, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 7244, 'Features': 512, 'Anomalies': 362, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 6028, 'Features': 512, 'Anomalies': 301, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5889, 'Features': 512, 'Anomalies': 294, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 5310, 'Features': 512, 'Anomalies': 265, 'Anomalies Ratio(%)': 4.99}\n",
      "current noise type: None\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 4904, 'Features': 512, 'Anomalies': 245, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "current noise type: None\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "current noise type: None\n",
      "{'Samples': 3090, 'Features': 768, 'Anomalies': 154, 'Anomalies Ratio(%)': 4.98}\n",
      "current noise type: None\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "current noise type: None\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "current noise type: None\n",
      "{'Samples': 2514, 'Features': 768, 'Anomalies': 125, 'Anomalies Ratio(%)': 4.97}\n",
      "current noise type: None\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "current noise type: None\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "current noise type: None\n",
      "{'Samples': 2497, 'Features': 768, 'Anomalies': 124, 'Anomalies Ratio(%)': 4.97}\n",
      "generating duplicate samples for dataset 20news_3...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 40, 'Anomalies Ratio(%)': 4.0}\n",
      "generating duplicate samples for dataset 20news_3...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "generating duplicate samples for dataset 20news_3...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 768, 'Anomalies': 44, 'Anomalies Ratio(%)': 4.4}\n",
      "current noise type: None\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "current noise type: None\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "current noise type: None\n",
      "{'Samples': 1657, 'Features': 768, 'Anomalies': 82, 'Anomalies Ratio(%)': 4.95}\n",
      "current noise type: None\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "current noise type: None\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "current noise type: None\n",
      "{'Samples': 1532, 'Features': 768, 'Anomalies': 76, 'Anomalies Ratio(%)': 4.96}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "current noise type: None\n",
      "{'Samples': 10000, 'Features': 768, 'Anomalies': 500, 'Anomalies Ratio(%)': 5.0}\n",
      "121 datasets, 7 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:04<00:26,  4.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GANomaly, AUC-ROC: 0.6776532630191167, AUC-PR: 0.4226436699084896\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: GANomaly, metrics: {'aucroc': 0.6776532630191167, 'aucpr': 0.4226436699084896}, fitting time: 4.285346746444702, inference time: 0.003116607666015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████                                                            | 2/7 [00:08<00:20,  4.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in model fitting. Model:DeepSAD, Error: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: DeepSAD, metrics: {'aucroc': nan, 'aucpr': nan}, fitting time: None, inference time: None\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method tripletRankingLossLayer.call of <adbench.baseline.REPEN.model.tripletRankingLossLayer object at 0x000001F6846BFF48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method tripletRankingLossLayer.call of <adbench.baseline.REPEN.model.tripletRankingLossLayer object at 0x000001F6846BFF48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method tripletRankingLossLayer.call of <adbench.baseline.REPEN.model.tripletRankingLossLayer object at 0x000001F6846BFF48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n",
      "\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:08<00:10,  2.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in model fitting. Model:REPEN, Error: 'a' cannot be empty unless no samples are taken\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: REPEN, metrics: {'aucroc': nan, 'aucpr': nan}, fitting time: None, inference time: None\n",
      "Training size: 700, No. outliers: 0\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DevNet.deviation_loss of <adbench.baseline.DevNet.run.DevNet object at 0x000001F6847E6F48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DevNet.deviation_loss of <adbench.baseline.DevNet.run.DevNet object at 0x000001F6847E6F48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method DevNet.deviation_loss of <adbench.baseline.DevNet.run.DevNet object at 0x000001F6847E6F48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:09<00:05,  1.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in model fitting. Model:DevNet, Error: a must be greater than 0 unless no samples are taken\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: DevNet, metrics: {'aucroc': nan, 'aucpr': nan}, fitting time: None, inference time: None\n",
      "Error in model fitting. Model:PReNet, Error: 'a' cannot be empty unless no samples are taken\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: PReNet, metrics: {'aucroc': nan, 'aucpr': nan}, fitting time: None, inference time: None\n",
      "autoencoder pre-training start....\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0617\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0514\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0414\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0332\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0275\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0245\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0232\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0222\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0212\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0203\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 29ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0196\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0190\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0184\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0179\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0174\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0169\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0164\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0161\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0158\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0153\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0147\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0144\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0143\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0141\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0139\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0134\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0132\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0130\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0128\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0125\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0120\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0116\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0114\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0103\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0102\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0101\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0101\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0100\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0100\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0090\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0089\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0089\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0089\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0088\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0088\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0088\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0088\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0087\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0087\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0087\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0087\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0087\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0087\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 1s 29ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0086\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0085\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0084\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0083\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0083\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0083\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0083\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0083\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0083\n",
      "load pretrained autoencoder model....\n",
      "load autoencoder model\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F6849ABCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F6849ABCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F6849ABCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "end-to-end training start....\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [01:13<00:17, 17.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in model fitting. Model:FEAWAD, Error: a must be greater than 0 unless no samples are taken\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: FEAWAD, metrics: {'aucroc': nan, 'aucpr': nan}, fitting time: None, inference time: None\n",
      "best param: None\n",
      "[22:28:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:36<00:00, 13.84s/it]\u001b[A\n",
      "1it [01:36, 96.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBOD, AUC-ROC: 0.5, AUC-PR: 0.13666666666666666\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.0, 1), model: XGBOD, metrics: {'aucroc': 0.5, 'aucpr': 0.13666666666666666}, fitting time: 19.388826370239258, inference time: 3.835327625274658\n",
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:05<00:34,  5.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GANomaly, AUC-ROC: 0.6789716545814106, AUC-PR: 0.4258012102570777\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: GANomaly, metrics: {'aucroc': 0.6789716545814106, 'aucpr': 0.4258012102570777}, fitting time: 5.548740863800049, inference time: 0.0009968280792236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████                                                            | 2/7 [00:10<00:26,  5.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DeepSAD, AUC-ROC: 0.6319804124682173, AUC-PR: 0.32975017827769154\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: DeepSAD, metrics: {'aucroc': 0.6319804124682173, 'aucpr': 0.32975017827769154}, fitting time: 4.71669864654541, inference time: 0.006981372833251953\n",
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n",
      "\n",
      " 43%|████████████████████████████████████                                                | 3/7 [02:28<04:22, 65.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: REPEN, AUC-ROC: 0.7720124305490159, AUC-PR: 0.4967824790336119\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: REPEN, metrics: {'aucroc': 0.7720124305490159, 'aucpr': 0.4967824790336119}, fitting time: 137.25167775154114, inference time: 0.06506991386413574\n",
      "Training size: 700, No. outliers: 1\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 11ms/step - batch: 9.5000 - size: 512.0000 - loss: 1.8317\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7276\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5189\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4711\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.3933\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.3739\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.3429\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.3237\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.3040\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2938\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2902\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2795\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 23ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2749\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 1s 26ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2732\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 24ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2609\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2625\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2571\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2516\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2478\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2408\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2432\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2360\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2362\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2338\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2275\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2235\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2275\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2209\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2185\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2154\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2152\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2139\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2094\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2131\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2129\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2082\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2061\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2058\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.2030\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1998\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1974\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1938\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1975\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1973\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1950\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1957\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1913\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 1s 27ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1905\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 1s 26ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1881\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 25ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.1926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DevNet, AUC-ROC: 0.6871645164328091, AUC-PR: 0.43072885264654387\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: DevNet, metrics: {'aucroc': 0.6871645164328091, 'aucpr': 0.43072885264654387}, fitting time: 20.429776191711426, inference time: 0.28744983673095703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████                                    | 4/7 [02:48<02:23, 47.96s/it]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [03:03<01:11, 35.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PReNet, AUC-ROC: 0.706187023260194, AUC-PR: 0.4542324538098336\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: PReNet, metrics: {'aucroc': 0.706187023260194, 'aucpr': 0.4542324538098336}, fitting time: 13.917425870895386, inference time: 0.322216272354126\n",
      "autoencoder pre-training start....\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 23ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0624\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0491\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0390\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0323\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0283\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0259\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0242\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0230\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0214\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0197\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0189\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0184\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0179\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0174\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0169\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0164\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0160\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0157\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0154\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0152\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0149\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 1s 43ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0147\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0145\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0144\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0142\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0139\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0133\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 1s 45ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0129\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 1s 47ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0127\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 1s 44ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0127\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0126\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0125\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0124\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0124\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0123\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0123\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0122\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0122\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0121\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0121\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0120\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0119\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0114\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0113\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0113\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 2s 94ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 1s 45ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 60ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 1s 57ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "load pretrained autoencoder model....\n",
      "load autoencoder model\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694EA9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694EA9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694EA9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "end-to-end training start....\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 2s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 4.8019\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 3.8934\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.9228\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6737\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5845\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5524\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5470\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5345\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5370\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5348\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5256\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5308\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5317\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5246\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5332\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5434\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5563\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5327\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5216\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5184\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 1s 42ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5178\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5218\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5194\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5284\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5251\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5232\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5224\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5213\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5204\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5204\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F684669D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F684669D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F684669D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [04:38<00:55, 55.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FEAWAD, AUC-ROC: 0.6276485544778228, AUC-PR: 0.3956861007701136\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: FEAWAD, metrics: {'aucroc': 0.6276485544778228, 'aucpr': 0.3956861007701136}, fitting time: 94.03020930290222, inference time: 0.4935309886932373\n",
      "best param: None\n",
      "[22:33:52] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [04:56<00:00, 42.37s/it]\u001b[A\n",
      "4it [06:33, 98.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBOD, AUC-ROC: 0.818156135229306, AUC-PR: 0.4698801490073006\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.01, 1), model: XGBOD, metrics: {'aucroc': 0.818156135229306, 'aucpr': 0.4698801490073006}, fitting time: 14.525497674942017, inference time: 3.926608085632324\n",
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:04<00:28,  4.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GANomaly, AUC-ROC: 0.6799133628401921, AUC-PR: 0.4252536559113965\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: GANomaly, metrics: {'aucroc': 0.6799133628401921, 'aucpr': 0.4252536559113965}, fitting time: 4.517500877380371, inference time: 0.001995086669921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████                                                            | 2/7 [00:10<00:25,  5.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DeepSAD, AUC-ROC: 0.6753931631980412, AUC-PR: 0.3829477306542615\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: DeepSAD, metrics: {'aucroc': 0.6753931631980412, 'aucpr': 0.3829477306542615}, fitting time: 5.241185188293457, inference time: 0.006972074508666992\n",
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n",
      "\n",
      " 43%|████████████████████████████████████                                                | 3/7 [02:41<04:47, 71.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: REPEN, AUC-ROC: 0.897259628966946, AUC-PR: 0.6110271562521038\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: REPEN, metrics: {'aucroc': 0.897259628966946, 'aucpr': 0.6110271562521038}, fitting time: 150.9487271308899, inference time: 0.06582355499267578\n",
      "Training size: 700, No. outliers: 5\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 9ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.1548\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 1s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 1.1333\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8675\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7896\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 1s 25ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7463\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 1s 51ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7092\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 1s 25ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6720\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 24ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6608\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6409\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6114\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6210\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6016\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5923\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5674\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 1s 26ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5597\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5618\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5345\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5290\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5100\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5124\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5045\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5090\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4974\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 23ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4963\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4980\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.5098\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4818\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4776\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4781\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4672\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4644\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 24ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4662\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4629\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4528\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4453\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4456\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4427\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4443\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4414\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4335\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4317\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4334\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4415\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4319\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4293\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4318\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4257\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4259\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4205\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.4272\n",
      "Model: DevNet, AUC-ROC: 0.7277521423862887, AUC-PR: 0.5195553289050955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [03:03<02:37, 52.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: DevNet, metrics: {'aucroc': 0.7277521423862887, 'aucpr': 0.5195553289050955}, fitting time: 22.22620987892151, inference time: 0.17387866973876953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [03:16<01:15, 37.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PReNet, AUC-ROC: 0.7679630850362558, AUC-PR: 0.48912843624853225\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: PReNet, metrics: {'aucroc': 0.7679630850362558, 'aucpr': 0.48912843624853225}, fitting time: 12.098674774169922, inference time: 0.2314913272857666\n",
      "autoencoder pre-training start....\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0636\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0503\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0403\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0334\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0290\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0267\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0252\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0240\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 54ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0226\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0216\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0202\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0192\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0186\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0181\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0175\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0167\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0160\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 1s 49ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0157\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0154\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 1s 54ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0152\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 1s 55ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0150\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0147\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0144\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0142\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0141\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0139\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0138\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0137\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 1s 46ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0136\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 1s 48ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0135\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0134\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0133\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0128\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0125\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0123\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0122\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0121\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0121\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0120\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0120\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0119\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0118\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0117\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0117\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0116\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 1s 44ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0116\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 2s 79ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0116\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0115\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0115\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0115\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 1s 56ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0114\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0114\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0113\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0113\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0113\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0113\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 1s 43ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0112\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 1s 58ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 1s 51ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "load pretrained autoencoder model....\n",
      "load autoencoder model\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F684669F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F684669F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F684669F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "end-to-end training start....\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 2s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 4.8671\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 4.0831\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 3.1197\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 1s 47ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.8018\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.7010\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6571\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6359\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6142\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5937\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5787\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5712\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5619\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5532\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5478\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5490\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5435\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5431\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5401\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5323\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5315\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5282\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5383\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 1s 51ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5431\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5304\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5260\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5324\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5312\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5299\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5309\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5318\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694DF9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694DF9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694DF9A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [04:53<00:58, 58.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FEAWAD, AUC-ROC: 0.7073170731707317, AUC-PR: 0.5180465405219695\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: FEAWAD, metrics: {'aucroc': 0.7073170731707317, 'aucpr': 0.5180465405219695}, fitting time: 96.94216871261597, inference time: 0.5184636116027832\n",
      "best param: None\n",
      "[22:39:07] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [05:15<00:00, 45.08s/it]\u001b[A\n",
      "7it [11:49, 102.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBOD, AUC-ROC: 0.8479612016197382, AUC-PR: 0.5509132372861963\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.05, 1), model: XGBOD, metrics: {'aucroc': 0.8479612016197382, 'aucpr': 0.5509132372861963}, fitting time: 17.21400284767151, inference time: 4.338435411453247\n",
      "generating duplicate samples for dataset MVTec-AD_hazelnut...\n",
      "current noise type: None\n",
      "{'Samples': 1000, 'Features': 512, 'Anomalies': 138, 'Anomalies Ratio(%)': 13.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:05<00:33,  5.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GANomaly, AUC-ROC: 0.6804783877954611, AUC-PR: 0.4274251181939922\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.1, 1), model: GANomaly, metrics: {'aucroc': 0.6804783877954611, 'aucpr': 0.4274251181939922}, fitting time: 5.402167558670044, inference time: 0.001992464065551758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|████████████████████████                                                            | 2/7 [00:12<00:31,  6.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DeepSAD, AUC-ROC: 0.7105188812505886, AUC-PR: 0.48961943262915114\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.1, 1), model: DeepSAD, metrics: {'aucroc': 0.7105188812505886, 'aucpr': 0.48961943262915114}, fitting time: 6.5796284675598145, inference time: 0.007976293563842773\n",
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output triplet_ranking_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_ranking_loss_layer.\n",
      "\n",
      " 43%|████████████████████████████████████                                                | 3/7 [02:25<04:17, 64.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: REPEN, AUC-ROC: 0.9090309822017139, AUC-PR: 0.7396891409427643\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.1, 1), model: REPEN, metrics: {'aucroc': 0.9090309822017139, 'aucpr': 0.7396891409427643}, fitting time: 133.2978265285492, inference time: 0.06183624267578125\n",
      "Training size: 700, No. outliers: 10\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 9ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.1063\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 1.2827\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 1.1058\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 1.0163\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.9615\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.9420\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.9156\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.9053\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8704\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8569\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8395\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8352\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8307\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8209\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8145\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7908\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.8005\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7918\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7705\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7613\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 19ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7514\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7450\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7402\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7322\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7283\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7215\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7021\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7084\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6945\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 16ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7143\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.7006\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6780\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 21ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6739\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6492\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6745\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 16ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6801\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 16ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6585\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6678\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 16ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6721\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6588\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6490\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6381\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 23ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6315\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 20ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6306\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6269\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6117\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6140\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 18ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6126\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 17ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.6110\n",
      "Model: DevNet, AUC-ROC: 0.843864770694039, AUC-PR: 0.5590608911813153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [02:44<02:18, 46.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.1, 1), model: DevNet, metrics: {'aucroc': 0.843864770694039, 'aucpr': 0.5590608911813153}, fitting time: 18.242757081985474, inference time: 0.16060996055603027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [02:56<01:07, 33.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: PReNet, AUC-ROC: 0.8071381486015632, AUC-PR: 0.5484106772445562\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.1, 1), model: PReNet, metrics: {'aucroc': 0.8071381486015632, 'aucpr': 0.5484106772445562}, fitting time: 11.786272525787354, inference time: 0.25879669189453125\n",
      "autoencoder pre-training start....\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 22ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0630\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0498\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0397\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0331\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0289\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0259\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 52ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0233\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 56ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0218\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0209\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 60ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0202\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0190\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0179\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0173\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0167\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0162\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0158\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0154\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0150\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0139\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0130\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0126\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 1s 51ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0124\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 1s 42ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0122\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0121\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0119\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0118\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0116\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0115\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0114\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0111\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0110\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0109\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0108\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0107\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0106\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 1s 53ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0105\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0104\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 1s 52ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0103\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0103\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0103\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0102\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0102\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0102\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0101\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 1s 51ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0101\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0101\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0100\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0100\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0100\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0100\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0099\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 1s 43ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 1s 50ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0098\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 1s 35ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0097\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0096\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 1s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0095\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 1s 33ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 1s 32ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 1s 34ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 0.0094\n",
      "load pretrained autoencoder model....\n",
      "load autoencoder model\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F6955B10D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F6955B10D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F6955B10D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "end-to-end training start....\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 2s 30ms/step - batch: 9.5000 - size: 512.0000 - loss: 4.8497\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 1s 31ms/step - batch: 9.5000 - size: 512.0000 - loss: 4.1085\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 1s 43ms/step - batch: 9.5000 - size: 512.0000 - loss: 3.3533\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 1s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 3.0478\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 1s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.9364\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.8626\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 1s 46ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.8074\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.7558\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6882\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6565\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6427\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6326\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6174\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 1s 40ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6035\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 1s 43ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.6021\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5988\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5945\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5924\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 1s 38ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5882\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5860\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5843\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5774\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5760\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 1s 36ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5807\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 1s 39ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5768\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 1s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5767\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 1s 50ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5764\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 1s 48ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5727\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 1s 41ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5751\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 1s 37ms/step - batch: 9.5000 - size: 512.0000 - loss: 2.5769\n",
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694D96678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694D96678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function FEAWAD.dev_network_d.<locals>.multi_loss at 0x000001F694D96678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [04:32<00:54, 54.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FEAWAD, AUC-ROC: 0.7399001789245693, AUC-PR: 0.5019616156761139\n",
      "Current experiment parameters: ('MVTec-AD_hazelnut', 0.1, 1), model: FEAWAD, metrics: {'aucroc': 0.7399001789245693, 'aucpr': 0.5019616156761139}, fitting time: 94.71541905403137, inference time: 0.5480444431304932\n",
      "best param: None\n",
      "[22:44:01] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [04:49<00:48, 48.27s/it]\n",
      "9it [16:38, 110.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-778da3c08424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ADBench'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'semi-supervise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealistic_synthetic_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\ADBench\\adbench\\run.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, dataset, clf)\u001b[0m\n\u001b[0;32m    297\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ADBench\\adbench\\run.py\u001b[0m in \u001b[0;36mmodel_fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DAGMM'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[0mscore_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m                 \u001b[0mscore_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mtime_inference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ADBench\\adbench\\baseline\\PyOD.py\u001b[0m in \u001b[0;36mpredict_score\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;31m# from pyod: for consistency, outliers are assigned with larger anomaly scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pyod\\models\\xgbod.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# construct the new feature space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mX_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_new_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_add\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pyod\\models\\xgbod.py\u001b[0m in \u001b[0;36m_generate_new_features\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardization_flag_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m                 \u001b[0mX_add\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pyod\\models\\knn.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;31m# get the distance of the current point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mdist_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m             \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_dist_by_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mpred_score_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from adbench.run import RunPipeline\n",
    "\n",
    "'''\n",
    "Params:\n",
    "suffix: file name suffix;\n",
    "\n",
    "parallel: running either 'unsupervise', 'semi-supervise', or 'supervise' (AD) algorithms,\n",
    "corresponding to the Angle I: Availability of Ground Truth Labels (Supervision);\n",
    "\n",
    "realistic_synthetic_mode: testing on 'local', 'global', 'dependency', and 'cluster' anomalies, \n",
    "corresponding to the Angle II: Types of Anomalies;\n",
    "\n",
    "noise type: evaluating algorithms on 'duplicated_anomalies', 'irrelevant_features' and 'label_contamination',\n",
    "corresponding to the Angle III: Model Robustness with Noisy and Corrupted Data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RunPipeline(suffix='ADBench', parallel='semi-supervise', realistic_synthetic_mode=None, noise_type=None)\n",
    "# return the results including [params, model_name, metrics, time_fit, time_inference]\n",
    "# besides, results will be automatically saved in the dataframe and ouputted as csv file in adbench/result folder\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RunPipeline(suffix='ADBench', parallel='unsupervise', realistic_synthetic_mode='cluster', noise_type=None)\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RunPipeline(suffix='ADBench', parallel='supervise', realistic_synthetic_mode=None, noise_type='irrelevant_features')\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your customized algorithm on ADBench datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized model on ADBench's datasets\n",
    "from adbench.baseline.Customized.run import Customized\n",
    "results = pipeline.run(clf=Customized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your customized algorithm on customized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized model on customized dataset\n",
    "import numpy as np\n",
    "dataset = {}\n",
    "dataset['X'] = np.random.randn(1000, 20)\n",
    "dataset['y'] = np.random.choice([0, 1], 1000)\n",
    "results = pipeline.run(dataset=dataset, clf=Customized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import AD algorithms from ADBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.random.randn(1000, 20)\n",
    "y_train = np.random.choice([0, 1], 1000)\n",
    "X_test = np.random.randn(100, 20)\n",
    "\n",
    "# Directly import AD algorithms from the existing toolkits like PyOD\n",
    "from adbench.baseline.PyOD import PYOD\n",
    "model = PYOD(seed=42, model_name='XGBOD')  # initialization\n",
    "model.fit(X_train, y_train)  # fit\n",
    "score = model.predict_score(X_test)  # predict\n",
    "\n",
    "# Import deep learning AD algorithms from our ADBench\n",
    "from adbench.baseline.PReNet.run import PReNet\n",
    "model = PReNet(seed=42)\n",
    "model.fit(X_train, y_train)  # fit\n",
    "score = model.predict_score(X_test)  # predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
